{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f68df0-4627-42a4-beb5-1f47b13d3133",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LynxDataset tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346f0e6-79bb-4d10-bdc6-06366510f498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup notebook and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901468f-6a87-4bf8-a666-b1adc20b8890",
   "metadata": {},
   "source": [
    "For now, I tested everything in pytorch 2.0.1.\n",
    "\n",
    "I had to install albumentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab73dfe-f60f-443f-9c67-34ca303a5c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Allow reloading of libraries without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8083f535-104d-4500-a7a6-037e8cb2b614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfslocalsup/pub/anaconda-py3/2023.03/envs/pytorch-gpu-2.0.1+py3.10.12/lib/python3.10/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "/gpfslocalsup/pub/anaconda-py3/2023.03/envs/pytorch-gpu-2.0.1+py3.10.12/lib/python3.10/site-packages/torchvision-0.15.2a0+fa99a53-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/gpfs7kro/gpfslocalsup/pub/anaconda-py3/2023.03/envs/pytorch-gpu-2.0.1+py3.10.12/lib/python3.10/site-packages/torchvision-0.15.2a0+fa99a53-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from lynx_id.data.triplets import LynxDataset\n",
    "from pathlib import Path\n",
    "dataset_csv = Path('/gpfsscratch/rech/ads/commun/datasets/extracted/lynx_dataset_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b8888-5b2b-4634-9ad4-3382b433f165",
   "metadata": {},
   "source": [
    "## Accessing an element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58295a81-df0b-4332-9133-618481d9bc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the dataset\n",
    "dataset = LynxDataset(dataset_csv=dataset_csv, loader=\"opencv\")\n",
    "\n",
    "input, output = dataset[0]  # Example for getting the first item\n",
    "\n",
    "# Accessing data\n",
    "image = input['image']\n",
    "lynx_id = output['lynx_id']\n",
    "# Access other metadata from input as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a32bf-cc49-4263-aaa5-2a735c4358d4",
   "metadata": {},
   "source": [
    "## Iterating through the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa1a87-16ea-42a8-879c-94c2cd656f93",
   "metadata": {},
   "source": [
    "Using opencv as a loader, we encounter issues when loading some images.\n",
    "\n",
    "Using pil with LOAD_TRUNCATED_IMAGES, we can circumvent this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2082abd-8c5a-4378-befb-d6c00fc078a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:   7%|▋         | 343/4743 [01:00<12:30,  5.87it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  10%|▉         | 472/4743 [01:29<19:47,  3.60it/s]libpng error: Read Error\n",
      "Processing dataset:  10%|▉         | 474/4743 [01:30<22:17,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 472: Image not found or corrupted at /gpfsscratch/rech/ads/commun/datasets/extracted/Deep learning lynx - data/0_dataset_raw/0_dataset_Marie_OFB_ocelles/OFB_OCELLES_1376/1376_OFB_OCELLES_2020-06-15_NA_23.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:  40%|███▉      | 1895/4743 [07:56<06:23,  7.43it/s] libpng error: Read Error\n",
      "Processing dataset:  40%|███▉      | 1896/4743 [07:57<10:55,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 1895: Image not found or corrupted at /gpfsscratch/rech/ads/commun/datasets/extracted/Deep learning lynx - data/0_dataset_raw/0_dataset_Marie_OFB_ocelles/OFB_OCELLES_F25-067=1376/F25-067=1376_OFB_OCELLES_2020-06-15_Orchamps-Vennes_29.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:  56%|█████▋    | 2678/4743 [11:04<06:32,  5.26it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  58%|█████▊    | 2766/4743 [11:21<07:01,  4.69it/s]Corrupt JPEG data: 64120 extraneous bytes before marker 0xd5\n",
      "Processing dataset:  70%|███████   | 3331/4743 [13:36<05:14,  4.49it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  84%|████████▎ | 3965/4743 [16:44<02:45,  4.71it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  84%|████████▎ | 3967/4743 [16:45<02:41,  4.81it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  92%|█████████▏| 4381/4743 [19:31<01:51,  3.24it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  93%|█████████▎| 4407/4743 [19:45<01:58,  2.84it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset:  96%|█████████▌| 4535/4743 [20:39<00:48,  4.27it/s]Corrupt JPEG data: 33 extraneous bytes before marker 0xd7\n",
      "Processing dataset:  96%|█████████▌| 4541/4743 [20:41<00:40,  4.99it/s]Corrupt JPEG data: premature end of data segment\n",
      "Processing dataset: 100%|██████████| 4743/4743 [22:26<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset iteration completed with checks.\n",
      "Corrupted indices: [472, 1895]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dataset = LynxDataset(dataset_csv=dataset_csv, loader=\"opencv\") # Need to use opencv to have the issues and detect them\n",
    "corrupted_indices = []  # List to store indices of corrupted images\n",
    "\n",
    "# Using tqdm to add a progress bar to the loop\n",
    "for idx in tqdm(range(len(dataset)), desc=\"Processing dataset\"):\n",
    "    try:\n",
    "        # Attempt to load the image data and other information\n",
    "        input_dict, output_dict = dataset[idx]\n",
    "\n",
    "        # Access key elements to ensure they're loaded correctly\n",
    "        _ = input_dict['image']\n",
    "        _ = output_dict['lynx_id']\n",
    "        # Add checks for other elements if necessary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "        corrupted_indices.append(idx)  # Append the index to the list\n",
    "        continue\n",
    "\n",
    "print(\"Dataset iteration completed with checks.\")\n",
    "print(f\"Corrupted indices: {corrupted_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0f38e-bfc7-45bb-ac1b-0ee16efa3fa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028e82e-4dd9-4ace-93c9-5db968d53c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image,ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def show_corrupted_images(dataset, corrupted_indices, num_images=5):\n",
    "    \"\"\"Function to display corrupted images.\"\"\"\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "\n",
    "    for i, idx in enumerate(corrupted_indices[:num_images]):\n",
    "        try:\n",
    "            # Load the image\n",
    "            img = Image.open(dataset.dataframe.iloc[idx]['filepath'])\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'Index: {idx}')\n",
    "            axes[i].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[i].set_title(f'Error at index {idx}')\n",
    "            axes[i].axis('off')\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your dataset and corrupted indices\n",
    "show_corrupted_images(dataset, corrupted_indices, num_images=len(corrupted_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50265e4b-7d5e-4bf2-9185-c463274a6d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(dataset.dataframe.iloc[2031]['filepath'])\n",
    "img = np.array(img)\n",
    "print(img.shape) # Alpha channel\n",
    "\n",
    "# This slices the image to only use the first three channels (RGB)\n",
    "rgb_img = img[:, :, 0:3]\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(rgb_img)\n",
    "plt.axis('off')  # Turn off axis numbers and labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cb0ac-7c3f-41b1-8c70-04729ad3739d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing loading with pil vs opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fd9b9-dd78-47c5-8a79-a49e1eec1ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image,ImageFile\n",
    "\n",
    "dataset_pil = LynxDataset(dataset_csv=dataset_csv, loader='pil')  # Use PIL\n",
    "dataset_opencv = LynxDataset(dataset_csv=dataset_csv, loader='opencv')  # Use OpenCV\n",
    "\n",
    "def measure_performance(dataset, num_samples=100):\n",
    "    start_time = time.time()\n",
    "    for i in range(num_samples):\n",
    "        _ = dataset[i]\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Measure performance\n",
    "\n",
    "_ = measure_performance(dataset_pil) #just for fairness, avoid cache difference...\n",
    "pil_time = measure_performance(dataset_pil)\n",
    "opencv_time = measure_performance(dataset_opencv)\n",
    "\n",
    "print(f\"Time taken with PIL: {pil_time} seconds\")\n",
    "print(f\"Time taken with OpenCV: {opencv_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9777a-cf9e-4218-b51a-0daf0c10dddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9033c-3cae-4280-bc2f-0789e1abf486",
   "metadata": {},
   "source": [
    "I splitted transformations in two parts : \n",
    "- transforms : systematic changes to images to bring them to the same format\n",
    "- augmentations : random changes at each epoch to create artificial data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e396cc-ff38-4aaa-929f-9b12671617db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define a standard resize transformation\n",
    "transform_test = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=1024),  # Resize the longest side to 1024\n",
    "        # A.PadIfNeeded(min_height=1024, min_width=1024),  # Pad to make the image 1024x1024\n",
    "        # A.RandomSizedCrop (min_max_height=(192,224), height=224, width=224, w2h_ratio=1.0, interpolation=1, always_apply=False),\n",
    "    ],\n",
    "        # bbox_params=A.BboxParams(format='coco', label_fields=['lynx_id'])\n",
    ")\n",
    "\n",
    "#    ToTensorV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a2a2c-1b27-4184-8617-d5130043b0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = LynxDataset(dataset_csv=dataset_csv, loader=\"pil\", transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed51bb-f5ba-4d7a-b167-fc41984280a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_idx = 105\n",
    "for j in range(5):\n",
    "    # print(dataset[image_idx][0]['image'].shape)\n",
    "    # Display the image\n",
    "    plt.imshow(dataset[image_idx][0]['image'])\n",
    "    plt.axis('off')  # Turn off axis numbers and labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7fd10-217f-40da-8f90-1e1ad9b10a2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ## Testing augmentations -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186efe87-397d-469a-9e86-79f0016d3b8c",
   "metadata": {},
   "source": [
    "Making sure that each time we get different changes each time we call a data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10523940-2818-46db-892e-4981c0c3045f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=15, p=0.5),  # Small rotations\n",
    "    A.CoarseDropout(max_holes=8, max_height=8, max_width=8, min_holes=1, fill_value=0, p=0.5),  # Coarse dropout\n",
    "    # Add more augmentations as needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abadd2-b399-4b48-b036-2efa37636871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = LynxDataset(dataset_csv=dataset_csv, loader=\"pil\", augmentation = augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae7c08-002b-4cad-990a-a84abe2167f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(dataset[0][0]['image'].shape)\n",
    "    # Display the image\n",
    "    plt.imshow(dataset[0][0]['image'])\n",
    "    plt.axis('off')  # Turn off axis numbers and labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324df75f-a608-4c5b-bfa8-ac5dfc6c60dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing full transform pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e85b8e-8191-424c-9828-183162ce1bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lynx_id.data.transformations_and_augmentations import transforms, augments\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.utils import measure_performance\n",
    "\n",
    "dataset = LynxDataset(dataset_csv=dataset_csv, loader=\"pil\", transform=transforms, augmentation=augments)\n",
    "\n",
    "time_transforms = measure_performance(dataset)\n",
    "print(f\"Time taken with transforms: {time_transforms} seconds\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(dataset[0][0]['image'].shape)\n",
    "    # Display the image\n",
    "    plt.imshow(dataset[0][0]['image'])\n",
    "    plt.axis('off')  # Turn off axis numbers and labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23436ee8-e6f5-4439-b85b-cb648273e3fb",
   "metadata": {},
   "source": [
    "## Split pytorch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517dff6d-74fa-4000-9202-eae894e5de31",
   "metadata": {},
   "source": [
    "This is just for checking the dataset is working properly.\n",
    "Later on, we will have to do an equilibrated split on specific criterion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c114d0-101e-4eec-90aa-c2359c7661e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assume 'dataset' is your LynxDataset instance\n",
    "dataset_size = len(dataset)\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Randomly split the dataset into train, validation, and test\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c0373-8ee0-4bf9-95d2-18e3244a7ea4",
   "metadata": {},
   "source": [
    "## Split csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cfe8c-696f-4c4e-9279-0d57d82f0efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load full dataset as a pandas DataFrame\n",
    "full_dataset = pd.read_csv(dataset_csv)\n",
    "\n",
    "# Split the dataset into train+val and test sets\n",
    "train_val_set, test_set = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the train+val set into train and validation sets\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Save these to .csv files in the same directory as the original dataset\n",
    "train_set.to_csv(dataset_csv.parent / 'train_set.csv', index=False)\n",
    "val_set.to_csv(dataset_csv.parent / 'val_set.csv', index=False)\n",
    "test_set.to_csv(dataset_csv.parent / 'test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999135c9-9348-456f-8f42-8f1a85a23134",
   "metadata": {},
   "source": [
    "## Batch with Dataloader tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203d742-4df0-42c3-add7-c31d9a097e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def my_collate(batch):\n",
    "    # Initialize lists to gather all elements for each key\n",
    "    images = []\n",
    "    sources = []\n",
    "    patterns = []\n",
    "    dates = []\n",
    "    locations = []\n",
    "    image_numbers = []\n",
    "    lynx_ids = []\n",
    "\n",
    "    # Iterate over each item in the batch\n",
    "    for input_dict, output_dict in batch:\n",
    "        # Append data from input dictionary\n",
    "        images.append(input_dict['image'])  # List of images\n",
    "        sources.append(input_dict['source'])\n",
    "        patterns.append(input_dict['pattern'])\n",
    "        dates.append(input_dict['date'])\n",
    "        locations.append(input_dict['location'])\n",
    "        image_numbers.append(input_dict['image_number'])\n",
    "\n",
    "        # Append data from output dictionary\n",
    "        lynx_ids.append(output_dict['lynx_id'])\n",
    "\n",
    "    # Construct the batched input and output dictionaries\n",
    "    batched_input_dict = {\n",
    "        'images': images,\n",
    "        'sources': sources,\n",
    "        'patterns': patterns,\n",
    "        'dates': dates,\n",
    "        'locations': locations,\n",
    "        'image_numbers': image_numbers\n",
    "    }\n",
    "\n",
    "    batched_output_dict = {\n",
    "        'lynx_ids': lynx_ids\n",
    "    }\n",
    "\n",
    "    return batched_input_dict, batched_output_dict\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=my_collate)\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075e371-515f-4aa6-b9f6-3723c21fb710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(enumerate(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68e3a9-fcd6-43ca-84e6-48aba0d545e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, outputs = batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.1_py3.10.12",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.0.1_py3.10.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
